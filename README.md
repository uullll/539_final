## Branch Description
The code here is supplementary/experimental. It contains three Patient-Level self attention models. (1) multi-modal: resnet101 has two unfrozen heads (branches): one makes a dim = 500 embedding processed by a self-attention pooling module and the other converts into binary logits; two losses are computed (one for each branch endpoint) and the total loss is a weighted sum of the two losses;  (2) instance-based pooling: self-attention pooling module (gated or not gated) takes individual logits from a frozen resnet101 pretrained on ISIC 2020 dataset; (3) embedding-based pooling: unfrozen layer 4 and head of resnet101 outputs an embedding (dim = 500) that is processed by a self-attention pooling module (gated or not gated).

